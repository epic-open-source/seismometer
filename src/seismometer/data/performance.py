import logging
import sys
from numbers import Number
from pathlib import Path
from typing import Any, Callable, Optional, Tuple, Union

import numpy as np
import pandas as pd
from opentelemetry import metrics
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.metrics.export import ConsoleMetricExporter, Gauge, PeriodicExportingMetricReader
from sklearn.metrics import auc

from .confidence import PRConfidenceParam, ROCConfidenceParam, confidence_dict
from .decorators import export

logger = logging.getLogger("seismometer")

DEFAULT_RHO = 1 / 3

PathLike = Union[str, Path]
COUNTS = ["TP", "FP", "TN", "FN"]
PERCENTS = [f"{count} (%)" for count in COUNTS]
RATE_METRICS = ["Flag Rate"]
PERFORMANCE = ["Accuracy", "Sensitivity", "Specificity", "PPV", "NPV"]
WORKFLOW_METRICS = ["LR+", "NetBenefitScore", "NNE"]
THRESHOLD = "Threshold"
MONOTONIC_METRICS = ["Sensitivity", "Specificity", "Flag Rate"]
OVERALL_PERFORMANCE = ["Positives", "Prevalence", "AUROC", "AUPRC"]
STATNAMES = RATE_METRICS + PERFORMANCE + WORKFLOW_METRICS + COUNTS


@export
class MetricGenerator:
    RESERVED_NAMES = ["Count", "Cohort", "Class"]

    def __init__(
        self,
        metric_names: list[str],
        metric_fn: Callable[..., dict[str, float]],
        default_metrics: Optional[list[str]] = None,
        otlp_path: Optional[str] = None,
    ):
        """
        A class that generates metrics from a dataframe.
        Keeps track of available metric names as well as the function to call to generate them.
        Delegates the call to the metric function, and returns the results as a dictionary.

        Parameters
        ----------
        metric_names : list[str]
            List of metric names that can be generated by the metric function.
            The metric "Count" is reserved.
        metric_fn : Callable[[pd.DataFrame, list[str], ...], list[str], dict[str, float]]
            Function that generates metrics from a dataframe.
        otlp_path: where OpenTelemetry-generated metrics should be written.
            Set to None for stdout (dump to console)
        """
        if not metric_names:
            raise ValueError("metric_names must be a non-empty list of supported metrics")
        if not hasattr(metric_fn, "__call__"):
            raise ValueError("metric_fn must be a callable function")
        if restricted_names := [name for name in self.RESERVED_NAMES if name in metric_names]:
            raise ValueError(f"Reserved metric name found: {restricted_names}")
        self.metric_names = metric_names
        self.metric_fn = metric_fn
        self.default_metrics = default_metrics or metric_names
        if otlp_path is not None:
            self.exhaust_file_path = otlp_path
        self.setup_otel_metrics()

    def __del__(self):
        if self.exhaust_file_path is not None:
            self.otlp_exhaust.close()

    def setup_otel_metrics(self, metric_names=None):
        """This initializes the instruments for the
        various metrics we would like to collect, and
        also opens the file we are writing to.

        Parameters
        ----------
        metric_names : list[str], optional
            Which metrics to initialize instruments for
        """
        # The IO object to write OTel data into.
        self.otlp_exhaust: sys.IO
        if hasattr(self, "exhaust_file_path"):
            self.otlp_exhaust = open(self.exhaust_file_path, "r")
        else:
            self.otlp_exhaust = sys.stdout
        reader = PeriodicExportingMetricReader(ConsoleMetricExporter(out=self.otlp_exhaust))
        meterProvider = MeterProvider(metric_readers=[reader])
        metrics.set_meter_provider(meterProvider)
        # OpenTelemetry: use this new object to spawn new "Instruments" (measuring devices)
        self.meter = metrics.get_meter("Seismo-meter")
        # Keep it like this for now: just make an instrument for each metric we are measuring
        # TODO: worry about the type of each metric being measured
        # TODO: get better descriptions for each metric besides just the name
        # This is a map from metric name to corresponding instrument
        self.instruments: dict[str, Gauge] = {}
        if not metric_names:
            metric_names = self.metric_names
        for mn in metric_names:
            self.instruments[mn] = self.meter.create_gauge(mn, description=mn)

    def __call__(
        self,
        dataframe: pd.DataFrame,
        cohort: dict[str, tuple[Any]],
        metric_names: list[str] = None,
        record_metrics=True,
        **kwargs,
    ) -> dict[str, float]:
        """
        Generate metrics from a dataframe.

        Parameters
        ----------
        dataframe : pd.DataFrame
            The dataframe to generate metrics from.

        cohort: dict[str, tuple[Any]]
            Which cohort we are selecting on to generate these metrics.

        record_metrics: whether to generate OpenTelemetry metrics from this call.

        Returns
        -------
        dict[str, float]
            A dictionary of metric names and their values.
        """
        if not metric_names:
            metric_names = self.default_metrics
        elif not set(metric_names).issubset(self.metric_names):
            raise ValueError(f"Invalid metric names: {set(metric_names) - set(self.metric_names)}")
        if len(dataframe) == 0:
            # Universal defaults, if no data frame, return NaN
            return {name: np.nan for name in metric_names}
        full_metrics = self.delegate_call(dataframe, metric_names, cohort, **kwargs)
        filtered_metrics = {k: v for k, v in full_metrics.items() if k in metric_names}
        if record_metrics:
            self._populate_metrics(cohort, filtered_metrics)
        return filtered_metrics

    def delegate_call(
        self, dataframe: pd.DataFrame, metric_names: list[str], cohort: dict[str, tuple[Any]], **kwargs
    ) -> dict[str, float]:
        """
        Generate metrics from a dataframe.

        Parameters
        ----------
        dataframe : pd.DataFrame
            The dataframe to generate metrics from.

        metric_names : list[str]
            List of metric names to generate.

        cohort: dict[str, tuple[Any]]
            Which cohort we are selecting on.

        kwargs:
            anything additional the delegate needs.

        Returns
        -------
        dict[str, float]
            A dictionary of metric names and their values.
        """
        return self.metric_fn(dataframe, metric_names, **kwargs)

    def _populate_metrics(self, cohort, metrics):
        """Populate the OpenTelemetry instruments with data from
        the model.

        Parameters
        ----------
        metrics : dict[str, float].
            The actual data we are populating.
        """
        if metrics is None:
            # metrics = self()
            raise Exception()
        for name in self.instruments.keys():
            # I think metrics.keys() is a subset of self.instruments.keys()
            # but I am not 100% on it. So this stays for now.
            if name in metrics:
                self._log_to_instrument(cohort, self.instruments[name], metrics[name])

    def _log_to_instrument(self, cohort_info, instrument: Gauge, data):
        """Write information to a single instrument. We need this
        wrapper function because the data we are logging may be a
        type such as a series, in which case we need to log each
        entry separately or at least do some extra preprocessing.

        Parameters
        ----------
        cohort_info : dict[str, tuple[Any]]
            Which cohort we are logging a measurement from.
        instrument : opentelemetry.sdk.metrics.Gauge
            The OpenTelemetry Gauge that we are recording a
            measurement to. Should be one of self.instruments.
        data
            The data we are recording. Could conceivably be either
            a numeric value or a series.
        """

        def set_one_datapoint(value):
            instrument.set({"value": value, "parameters": cohort_info})

        if isinstance(data, (int, float)):
            set_one_datapoint(data)
        elif isinstance(data, pd.Series):
            for k, v in data.items():
                set_one_datapoint(v)
        else:
            raise Exception(f"Unrecognized data format for OTel logging: {type(data)}")

    def __repr__(self):
        return f"MetricGenerator(metric_names={self.metric_names}, metric_fn={self.metric_fn.__name__})"


@export
class BinaryClassifierMetricGenerator(MetricGenerator):
    def __init__(self, rho: float = None):
        """
        A class that generates Binary classifier metrics from a dataframe.
        Keeps track of available metric names as well as the function to call to generate them.
        Delegates the call to the metric function and returns the results as a dictionary.
        Cannot handle parametrized metrics (one value per threshold)

        rho : float, optional
            The relative risk reduction for NNT calculation, by default DEFAULT_RHO.
        """
        self.rho = rho or DEFAULT_RHO
        self.default_metrics = PERFORMANCE
        self.setup_otel_metrics(self.default_metrics)

    @property
    def metric_names(self):
        return STATNAMES + PERCENTS + [f"NNT@{self.rho:0.3n}"]

    def delegate_call(
        self,
        dataframe: pd.DataFrame,
        metric_names: list[str],
        cohort: dict[str, tuple[Any]],
        *,
        target_col: str,
        score_col: str,
        score_threshold: float = 0.5,
    ) -> dict[str, float]:
        """
        Generate metrics from a dataframe.

        Parameters
        ----------
        dataframe : pd.DataFrame
            The dataframe to generate metrics from.
        metric_names : list[str]
            List of metric names to generate.
        target_col : str
            The column in the dataframe that contains the true labels.
        score_col : str
            The column in the dataframe that contains the predicted scores.
        score_threshold : float, optional
            The threshold to use for binary classification, by default 0.5.

        Returns
        -------
        dict[str, float]
            A dictionary of metric names and their values.
        """
        stats = self.calculate_binary_stats(dataframe, target_col, score_col, metric_names, cohort)[0]
        score_threshold_integer = int(score_threshold * 100)
        stats = stats.loc[score_threshold_integer]
        return stats.to_dict()

    def calculate_binary_stats(self, dataframe, target_col, score_col, metrics, cohort, threshold_precision=0):
        """
        Calculates binary stats for all thresholds.

        Parameters
        ----------
        dataframe : pd.DataFrame
            The dataframe to generate metrics from.
        target_col : str
            The column in the dataframe that contains the true labels.
        score_col : str
            The column in the dataframe that contains the predicted scores.
        metrics: list[str], optional
            List of metrics to filter down to.
        cohort: dict[str, tuple[Any]]
            Which population we are selecting on, for logging purposes.
        threshold_precision : int, optional
            Number of decimal places to use when generating thresholds as percentages.
            - E.g., `threshold_precision=0` yields thresholds like 0, 1, ..., 100 (coarse).
            - `threshold_precision=2` yields 0.00, 0.01, ..., 100.00 (fine-grained).
            - Higher values improve AUC approximation but increase computation cost.
            By default 0.
        """
        y_true = dataframe[target_col]
        y_pred = dataframe[score_col]
        stats = (
            calculate_bin_stats(y_true, y_pred, rho=self.rho, threshold_precision=threshold_precision)
            .round(5)
            .set_index(THRESHOLD)
        )

        self._populate_metrics(cohort, stats)

        for name, percent in zip(COUNTS, PERCENTS):
            stats[percent] = stats[name] * 100.0 / len(dataframe)

        # Calculate overall statistics
        overall_stats = {}
        overall_stats["Positives"] = stats["TP"].iloc[-1]
        overall_stats["Prevalence"] = stats["TP"].iloc[-1] / len(dataframe)
        overall_stats["AUROC"] = auc(1 - stats["Specificity"], stats["Sensitivity"])
        overall_stats["AUPRC"] = auc(stats["Sensitivity"], stats["PPV"])

        if metrics:
            _metrics = [metric for metric in metrics if metric not in OVERALL_PERFORMANCE + [THRESHOLD]]
            return stats[list(_metrics)], overall_stats
        return stats, overall_stats

    def __repr__(self):
        return f"BinaryClassifierMetricGenerator(rho={self.rho})"


@export
def assert_valid_performance_metrics_df(df: pd.DataFrame, needs_columns: list = None) -> bool:
    """
    Determines whether a passed dataframe has either all or a subset of columns that likely indicate
    it was generated by calculate_bin_stats.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame under investigation.
    needs_columns : list
        List of columns that need to be present in order to be determined valid.
        Default is all columns normally generated by calculate_bin_stats.

    Returns
    -------
    bool
        Whether it is likely a valid performance metrics df.
    """
    performance_columns = needs_columns or [
        "Threshold",
        "Accuracy",
        "Sensitivity",
        "Specificity",
        "PPV",
        "NPV",
    ]
    if (df is None) or (not all([item in df.columns for item in performance_columns])):
        raise ValueError(
            "Passed performance frame does not have required columns: "
            + f"{performance_columns}.\nMissing {set(performance_columns) - set(df.columns)}"
        )


@export
def calculate_bin_stats(
    y_true: Optional[pd.Series] = None,
    y_pred: Optional[pd.Series] = None,
    keep_score_values: bool = False,
    not_point_thresholds: bool = False,
    rho: float = None,
    threshold_precision: int = 0,
) -> pd.DataFrame:
    """
    Calculate summary statistics from y_true and y_pred (y_proba[:,1] for binary classification) arrays.
    Supports y_true & y_pred as individual series-likes or as a dataframe with true and proba columns.

    Parameters
    ----------
    y_true : Optional[pd.Series], optional
        Series like of binary labels.
    y_pred : Optional[pd.Series], optional
        Series like of probabilities for positive class.
    keep_score_values : bool, optional
        Flag to prevent attempts to convert score to percentage (0-100), default False.
    not_point_thresholds : bool, optional
        If True, does not use point thresholds, by default False; uses 0-100.
    rho : float, optional
        The relative risk reduction for NNT calculation, by default DEFAULT_RHO.
    threshold_precision : int, optional
        Number of decimal places to use when generating thresholds as percentages.
        - E.g., `threshold_precision=0` yields thresholds like 0, 1, ..., 100 (coarse).
        - `threshold_precision=2` yields 0.00, 0.01, ..., 100.00 (fine-grained).
        - Higher values improve AUC approximation but increase computation cost.
        By default 0.

    Returns
    -------
    pd.DataFrame of stats, rows for each threshold value between 0 and 100 with columns for basic statistics.
    """
    rho = rho or DEFAULT_RHO
    y_true = y_true.astype(float)  # Expect numeric labels

    keep = ~(np.isnan(y_true) | np.isnan(y_pred))
    if not keep.any():
        return pd.DataFrame(columns=[THRESHOLD] + STATNAMES)

    # reduce
    y_true = y_true[keep]
    y_pred = y_pred[keep].round(5)

    n = len(y_true)
    total_positives = y_true.sum()
    total_negatives = n - total_positives

    if not keep_score_values:
        y_pred = as_percentages(y_pred)

    fps, tps, thresholds = _bin_class_curve(y_true, y_pred)

    # Add extrema if needed (logits); tree-likes could make predictions of 1 and 0
    if np.min(y_pred) > 0:
        thresholds = np.hstack((thresholds, [0]))
        tps = np.hstack((tps, total_positives))
        fps = np.hstack((fps, total_negatives))
    if (not keep_score_values) and (np.max(y_pred) < 100):
        thresholds = np.hstack(([100], thresholds))
        tps = np.hstack(([0], tps))
        fps = np.hstack(([0], fps))

    # Reduce thresholds to table 0-100
    # This can reintroduce redundant thresholds, particularly in sparse regions
    if not not_point_thresholds:
        threshold_ix, thresholds = _point_thresholds(thresholds, threshold_precision)
        tps = tps[threshold_ix]
        fps = fps[threshold_ix]

    # false negatives are any true positives that were not predicted positive
    fns = total_positives - tps

    # true negatives that were not predictived positive
    tns = total_negatives - fps

    with np.errstate(invalid="ignore", divide="ignore"):
        tpr = tps / total_positives
        fpr = fps / total_negatives

        ppv = tps / (tps + fps)
        ppv[np.isnan(ppv)] = 1

        # TN / TN + FN
        npv = np.divide(tns, tns + fns)
        npv[np.isnan(npv)] = 1

        lr = tpr / fpr

        # re-implementation of metrics from med_metrics package, see _calculate_nnt for full citation
        nne = calculate_nnt(ppv, 1)
        nnt = calculate_nnt(ppv, rho=rho)
        nbs = (tps - fps * (thresholds / (100 - thresholds))) / n

    accuracy = (tps + tns) / n
    ppcr = (tps + fps) / n

    # NOTE: Don't set index to be threshold because it's a float and this
    # makes lookup annoying due to tolerance settings
    # NOTE: Column order matters, so they are grouped by types
    stats = pd.DataFrame(
        np.column_stack(
            (
                thresholds,
                # RATES
                ppcr,  # Flag Rate
                # PERFORMANCE
                accuracy,
                tpr,  # Sensitivity
                1 - fpr,  # Specificity
                ppv,
                npv,
                # WORKFLOWS
                lr,
                nbs,
                nne,
                # COUNTS
                tps,
                fps,
                tns,
                fns,
                # DYNAMIC
                nnt,
            )
        ),
        columns=[THRESHOLD] + STATNAMES + [f"NNT@{rho:0.3n}"],
    )

    stats[COUNTS] = stats[COUNTS].fillna(0).astype(int)  # Strengthen dtypes on counts
    return stats


@export
def calculate_nnt(arr: np.ndarray, rho: Optional[Number | None] = None) -> np.ndarray:
    """
    Calculates NNT (Number Needed to Treat) for the relative risk reduction, rho, and a
    perfect-ARR (absolute risk reduction), ie PPV.

    This formulation and the related ARR and Net Benefit calculation is originally
    from eotles/med_metrics [#med_metrics]_.

    Parameters
    ----------
    arr : np.ndarray
        The array of absolute risk reductions for each threshold assuming a rho of 1
    rho : Number, optional
        The estimated relative risk reduction, by default is DEFAULT_RHO (1/3).

    Returns
    -------
    np.ndarray
        the NNT for each threshold.

    References
    ----------
    .. [#med_metrics] eotles/med_metrics: Initial public release. Zenodo; 2024.
           http://dx.doi.org/10.5281/ZENODO.10514448
    """
    rho = rho or DEFAULT_RHO

    # Divide by zero is ok
    with np.errstate(invalid="ignore", divide="ignore"):
        nnt = 1 / (rho * arr)

    return nnt


@export
def calculate_eval_ci(
    stats: pd.DataFrame, truth: pd.Series, output: pd.Series, conf: Number = 0.95, force_percentages=False
) -> dict:
    """
    Calculate confidence intervals for ROC, PR, and other performance metrics from a stats frame.

    Parameters
    ----------
    stats : pd.DataFrame
        The performance statistics, generated by calculate_bin_stats or similar.
    truth : pd.Series
        The series of data with the ground truth labeling that is associated with the stats frame.
    output : pd.Series
        The series of model output associated with the stats frame.
    conf : Number, optional
        The confidence level for calculation, by default 0.95.
    force_percentages : bool, optional
        Flag to indicate that outputs should be converted to percentages (0-100), by default False.

    Returns
    -------
    dict
        A dictionary of confidence details for an evaluation plot.
    """
    required_columns = ["TP", "FP", "TN", "FN", "Threshold", "Sensitivity", "PPV"]
    assert_valid_performance_metrics_df(stats, required_columns)

    conf = confidence_dict(conf)
    if truth is None or output is None:
        return conf, None, None, None

    if force_percentages:
        output = as_percentages(output)

    roc_conf = ROCConfidenceParam(conf)
    auprc_conf = PRConfidenceParam(conf)
    with np.errstate(invalid="ignore", divide="ignore"):
        # roc
        thresholds, tpr, fpr, auc_region = roc_conf.region(roc_conf, truth, output)
        auc_interval = roc_conf.interval(roc_conf, truth, output)
        # pr
        auprc_interval = auprc_conf.interval(
            auprc_conf,
            auc(stats.Sensitivity, stats.PPV),
            stats[["TP", "FP", "TN", "FN"]].iloc[0].sum(),
        )

    ci_data = {
        "roc": {
            "Threshold": thresholds,
            "TPR": tpr,
            "FPR": fpr,
            "region": auc_region,
            "interval": auc_interval,
        },
        "pr": {"interval": auprc_interval},
        "conf": conf,
    }
    return ci_data


def _bin_class_curve(
    y_true: Union[np.ndarray, pd.Series], y_pred: Union[np.ndarray, pd.Series]
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Inspired by binary_clf_curve.
    Does core calculations for determining performance stats.
    """
    # Sort both arrays to have predictions descending
    sort_ix = np.argsort(y_pred, kind="mergesort")[::-1]
    y_true = np.array(y_true)[sort_ix]
    y_pred = np.array(y_pred)[sort_ix]

    # Find where the threshold changes
    distinct_ix = np.where(np.diff(y_pred))[0]
    threshold_idxs = np.r_[distinct_ix, y_true.size - 1]

    # Add up the true positives and infer false ones
    tps = np.cumsum(y_true)[threshold_idxs]
    fps = 1 + threshold_idxs - tps

    return fps, tps, y_pred[threshold_idxs]


def _point_thresholds(orig_thresholds: np.ndarray, threshold_precision: int = 0) -> np.ndarray:
    """
    Convert float thresholds into evenly spaced percent thresholds between 0 and 100,
    with granularity based on the specified number of decimal places.
    """
    if orig_thresholds.max() < 1:
        logger.warning("Passed thresholds do not extend to maximum of 1.")

    # Compute number of divisions per unit based on precision
    step_size = 10**threshold_precision
    num_steps = 100 * step_size

    thresholds = (np.arange(0, num_steps + 1)[::-1]) / step_size  # Convert to percent scale
    ixs = np.digitize(thresholds, orig_thresholds, right=True) - 1
    ixs = np.where(ixs < 0, 0, ixs)  # Clip to 0
    ixs[-1] = -1  # Keep the last threshold
    return ixs, thresholds


def as_percentages(proba: np.ndarray) -> np.ndarray:
    """
    Converts a probability in the 0-1 range to a percentage in the 0-100 range.

    Parameters
    ----------
    proba : np.ndarray
        array-like list of probabilities.

    Returns
    -------
    np.ndarray
        array-like list of percentages.
    """
    if proba.max() < 2:
        proba *= 100
    return proba


def as_probabilities(perc: np.ndarray) -> np.ndarray:
    """
    Converts a percentage in the 0-100 range to a probability in the 0-1 range.

    Parameters
    ----------
    perc : np.ndarray
        array-like list of percentages.

    Returns
    -------
    np.ndarray
        array-like list of probabilities.
    """
    if perc.max() > 2:
        perc /= 100
    return perc
